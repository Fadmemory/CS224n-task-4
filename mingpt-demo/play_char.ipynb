{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:31:46.962655Z",
     "iopub.status.busy": "2025-11-11T06:31:46.962108Z",
     "iopub.status.idle": "2025-11-11T06:31:46.969878Z",
     "shell.execute_reply": "2025-11-11T06:31:46.968432Z",
     "shell.execute_reply.started": "2025-11-11T06:31:46.962634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:31:49.239281Z",
     "iopub.status.busy": "2025-11-11T06:31:49.239067Z",
     "iopub.status.idle": "2025-11-11T06:31:50.318184Z",
     "shell.execute_reply": "2025-11-11T06:31:50.317525Z",
     "shell.execute_reply.started": "2025-11-11T06:31:49.239263Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:31:52.941915Z",
     "iopub.status.busy": "2025-11-11T06:31:52.941607Z",
     "iopub.status.idle": "2025-11-11T06:31:52.944824Z",
     "shell.execute_reply": "2025-11-11T06:31:52.944003Z",
     "shell.execute_reply.started": "2025-11-11T06:31:52.941887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:31:55.200227Z",
     "iopub.status.busy": "2025-11-11T06:31:55.200003Z",
     "iopub.status.idle": "2025-11-11T06:31:55.204915Z",
     "shell.execute_reply": "2025-11-11T06:31:55.204167Z",
     "shell.execute_reply.started": "2025-11-11T06:31:55.200209Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size): # blocksize? \n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size #? why\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1] #why blocksize+1?\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk] # +1 because x = chunk[0:-1], y = chunk[1:]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:31:59.584681Z",
     "iopub.status.busy": "2025-11-11T06:31:59.584440Z",
     "iopub.status.idle": "2025-11-11T06:31:59.587219Z",
     "shell.execute_reply": "2025-11-11T06:31:59.586508Z",
     "shell.execute_reply.started": "2025-11-11T06:31:59.584661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context  \n",
    "# context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-11-11T05:53:41.051716Z",
     "shell.execute_reply": "2025-11-11T05:53:41.051001Z",
     "shell.execute_reply.started": "2025-11-11T05:53:33.058267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:07 --:--:--     0^C\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:32:02.546776Z",
     "iopub.status.busy": "2025-11-11T06:32:02.546521Z",
     "iopub.status.idle": "2025-11-11T06:32:02.561366Z",
     "shell.execute_reply": "2025-11-11T06:32:02.560740Z",
     "shell.execute_reply.started": "2025-11-11T06:32:02.546756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "from dataset import CharDataset\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T06:32:08.838353Z",
     "iopub.status.busy": "2025-11-11T06:32:08.838119Z",
     "iopub.status.idle": "2025-11-11T06:32:09.129702Z",
     "shell.execute_reply": "2025-11-11T06:32:09.128957Z",
     "shell.execute_reply.started": "2025-11-11T06:32:08.838335Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/11/2025 14:32:09 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T06:00:46.082789Z",
     "iopub.status.busy": "2025-11-11T06:00:46.082543Z",
     "iopub.status.idle": "2025-11-11T06:00:46.085363Z",
     "shell.execute_reply": "2025-11-11T06:00:46.084761Z",
     "shell.execute_reply.started": "2025-11-11T06:00:46.082769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-11T06:32:38.012195Z",
     "iopub.status.busy": "2025-11-11T06:32:38.011948Z",
     "iopub.status.idle": "2025-11-11T07:48:23.573067Z",
     "shell.execute_reply": "2025-11-11T07:48:23.572375Z",
     "shell.execute_reply.started": "2025-11-11T06:32:38.012173Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 5808: train loss 0.22509. lr 3.000169e-04: 100%|██████████| 5809/5809 [37:51<00:00,  2.56it/s]\n",
      "epoch 2 iter 5808: train loss 0.13475. lr 6.000000e-05: 100%|██████████| 5809/5809 [37:52<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=192, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T08:02:48.584300Z",
     "iopub.status.busy": "2025-11-11T08:02:48.584071Z",
     "iopub.status.idle": "2025-11-11T08:02:48.679535Z",
     "shell.execute_reply": "2025-11-11T08:02:48.678797Z",
     "shell.execute_reply.started": "2025-11-11T08:02:48.584282Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T07:58:41.566590Z",
     "iopub.status.busy": "2025-11-11T07:58:41.566349Z",
     "iopub.status.idle": "2025-11-11T07:58:49.811488Z",
     "shell.execute_reply": "2025-11-11T07:58:49.810814Z",
     "shell.execute_reply.started": "2025-11-11T07:58:41.566559Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! that e'er this tongue of mine,\n",
      "That laid the sentence of dread banishment\n",
      "On yon proud man, should take it off again\n",
      "With words of sooth! O that I were as great\n",
      "As is my grief, or lesser than my name!\n",
      "Or that I could forget what I have been,\n",
      "Or not remember what I must be now!\n",
      "Swell'st thou, proud heart? I'll give thee scope to beat,\n",
      "Since foes have scope to beat both thee and me.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Northumberland comes back from Bolingbroke.\n",
      "\n",
      "KING RICHARD II:\n",
      "What must the king do now? must he submit?\n",
      "The king shall do it: must he be deposed?\n",
      "The king shall be contented: must he lose\n",
      "The name of king? o' God's name, let it go:\n",
      "I'll give my jewels for a set of beads,\n",
      "My gorgeous palace for a hermitage,\n",
      "My gay apparel for an almsman's gown,\n",
      "My figured goblets for a dish of wood,\n",
      "My sceptre for a palmer's walking staff,\n",
      "My subjects for a pair of carved saints\n",
      "And my large kingdom for a little grave,\n",
      "A little little grave, an obscure grave;\n",
      "Or I'll be buried in the king's highway,\n",
      "Some way of common trade, where subjects' feet\n",
      "May hourly trample on their sovereign's head;\n",
      "For on my heart they tread now whilst I live;\n",
      "And buried once, why not upon my head?\n",
      "Aumerle, thou weep'st, my tender-hearted cousin!\n",
      "We'll make foul weather with despised tears;\n",
      "Our sighs and they shall lodge the summer corn,\n",
      "And make a dearth in this revolting land.\n",
      "Or shall we play the wantons with our woes,\n",
      "And make some pretty match with shedding tears?\n",
      "As thus, to drop them still upon one place,\n",
      "Till they have fretted us a pair of graves\n",
      "Within the earth; and, therein laid,--there lies\n",
      "Two kinsmen digg'd their graves with weeping eyes.\n",
      "Would not this ill do well? Well, well, I see\n",
      "I talk but idly, and you laugh at me.\n",
      "Most mighty prince, my Lord Northumberland,\n",
      "What says King Bolingbroke? will his majesty\n",
      "Give Richard leave to live till Richard die?\n",
      "You make a leg, and Bolingbroke says ay.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "My lord, in the base court he doth attend\n",
      "To speak with you; may it please you to come dow\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
